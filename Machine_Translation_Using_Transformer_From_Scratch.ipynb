{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1878727,
          "sourceType": "datasetVersion",
          "datasetId": 1118439
        },
        {
          "sourceId": 8311007,
          "sourceType": "datasetVersion",
          "datasetId": 4937140
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Machine Translation Using Transformer From Scratch",
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b033917262cf4175a597b13dddc08766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7147cb76c8c64132ab4f1f823d268ef3",
              "IPY_MODEL_e404811732a4485e9586ac6e483b9c07",
              "IPY_MODEL_7bd7e031e56a4a8bbc4dba215c1275cf"
            ],
            "layout": "IPY_MODEL_229466c012354d899584b252ebd6fbe5"
          }
        },
        "7147cb76c8c64132ab4f1f823d268ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_945da085a3024159991ebcb9e075aa42",
            "placeholder": "​",
            "style": "IPY_MODEL_071ff52b919f434cb03da907fea9e05d",
            "value": "100%"
          }
        },
        "e404811732a4485e9586ac6e483b9c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e6835c94ffd48afa27fe7a4d17fcb04",
            "max": 40000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6c150993ec04980a71db674a85c5ad7",
            "value": 40000
          }
        },
        "7bd7e031e56a4a8bbc4dba215c1275cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2066efac9f9343d5b090eb8f5c43d321",
            "placeholder": "​",
            "style": "IPY_MODEL_9c066e16906e43f487382f6e5d1ce400",
            "value": " 40000/40000 [00:00&lt;00:00, 149529.59it/s]"
          }
        },
        "229466c012354d899584b252ebd6fbe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "945da085a3024159991ebcb9e075aa42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "071ff52b919f434cb03da907fea9e05d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e6835c94ffd48afa27fe7a4d17fcb04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c150993ec04980a71db674a85c5ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2066efac9f9343d5b090eb8f5c43d321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c066e16906e43f487382f6e5d1ce400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba32e2e0c22746b69c061c0c8754b1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f80bea5036f4eff977507b41b7b7b1c",
              "IPY_MODEL_6560961986534e6094a3f75b033fbc57",
              "IPY_MODEL_5c8d9624fa8e4ef4b4de39cd68d46d97"
            ],
            "layout": "IPY_MODEL_c81aa6506ca74e68aac1120c499ae537"
          }
        },
        "7f80bea5036f4eff977507b41b7b7b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b62af005dbab4b91a8a4d791ad89aa54",
            "placeholder": "​",
            "style": "IPY_MODEL_d2a8839b7f8a4b7e9f2cc4e6b374027d",
            "value": "100%"
          }
        },
        "6560961986534e6094a3f75b033fbc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb7170f60b914db9b9ebbba8915a7d26",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f083ca73c244585a6c01c9253397150",
            "value": 5000
          }
        },
        "5c8d9624fa8e4ef4b4de39cd68d46d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd615bff20d49309d798d7b2fa949c7",
            "placeholder": "​",
            "style": "IPY_MODEL_60902767caec4565b43c33aceaf3a732",
            "value": " 5000/5000 [00:00&lt;00:00, 75609.28it/s]"
          }
        },
        "c81aa6506ca74e68aac1120c499ae537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b62af005dbab4b91a8a4d791ad89aa54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a8839b7f8a4b7e9f2cc4e6b374027d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb7170f60b914db9b9ebbba8915a7d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f083ca73c244585a6c01c9253397150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fd615bff20d49309d798d7b2fa949c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60902767caec4565b43c33aceaf3a732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df8817c980b14ef1aa8e634e163c21d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ded55bc7b098477580bfe50ee8677454",
              "IPY_MODEL_26316c32128844a0b0942a0fa221ecb7",
              "IPY_MODEL_b5523bcf53ae4fd8afdd975346553926"
            ],
            "layout": "IPY_MODEL_1dc61ef7cd744fd5baf13a37cb09435d"
          }
        },
        "ded55bc7b098477580bfe50ee8677454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_371f4fd70b3242f08c4b5bcac2672d04",
            "placeholder": "​",
            "style": "IPY_MODEL_5bece993c9394e7c9eba6bf696cdb2c3",
            "value": "100%"
          }
        },
        "26316c32128844a0b0942a0fa221ecb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0c46c88fbf84584b48ff90391dcf45d",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66cb1d1db5744bd1b0ba7f073a8bc5e0",
            "value": 5000
          }
        },
        "b5523bcf53ae4fd8afdd975346553926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1816bf4d8db84af98a9f299c3ef394b2",
            "placeholder": "​",
            "style": "IPY_MODEL_d164d527ca7945708a1e57a07a10d541",
            "value": " 5000/5000 [00:00&lt;00:00, 75919.68it/s]"
          }
        },
        "1dc61ef7cd744fd5baf13a37cb09435d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "371f4fd70b3242f08c4b5bcac2672d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bece993c9394e7c9eba6bf696cdb2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0c46c88fbf84584b48ff90391dcf45d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66cb1d1db5744bd1b0ba7f073a8bc5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1816bf4d8db84af98a9f299c3ef394b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d164d527ca7945708a1e57a07a10d541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "alincijov_bilingual_sentence_pairs_path = kagglehub.dataset_download('alincijov/bilingual-sentence-pairs')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "H5-ESrgp5x_s",
        "outputId": "702b5f28-6f7e-44ee-f561-1f3e9c8f733d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "In this notebook, I'll be focusing on machine translation using a Bengali to English dataset. I spent 10 days gathering the data, resulting in a clean dataset containing a total of 4 million rows. It's worth noting that this dataset is not yet public. Nonetheless, I'll attempt to train a Transformer architecture model for machine translation."
      ],
      "metadata": {
        "id": "PdMMJaOG5x_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "JVoYAwml5x_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.12.0"
      ],
      "metadata": {
        "id": "EGbeRjU9595H",
        "outputId": "d88a65b7-e590-4665-f568-3705910d71a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.12.0 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (2.32.3)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12.0) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchtext==0.12.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12.0) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchtext\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer\n",
        "from keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n",
        "from keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from tensorboard.plugins import projector"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:34:37.081741Z",
          "iopub.execute_input": "2024-05-08T10:34:37.082163Z",
          "iopub.status.idle": "2024-05-08T10:34:58.124717Z",
          "shell.execute_reply.started": "2024-05-08T10:34:37.082134Z",
          "shell.execute_reply": "2024-05-08T10:34:58.123554Z"
        },
        "trusted": true,
        "id": "33gggLcf5x_v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('en-fr.txt', names=['en', 'fr', 'attr'], usecols=['en', 'fr'], sep='\\t')\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "twydJs2P8Zxx",
        "outputId": "ef52888e-b464-4424-a96b-a5e8a0d6ebda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     en                                fr\n",
              "0   You're very clever.        Vous êtes fort ingénieuse.\n",
              "1       Are there kids?            Y a-t-il des enfants ?\n",
              "2              Come in.                          Entrez !\n",
              "3       Where's Boston?                   Où est Boston ?\n",
              "4  You see what I mean?  Vous voyez ce que je veux dire ?"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c43aeb9-1327-40c5-9f5b-917843f83712\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You're very clever.</td>\n",
              "      <td>Vous êtes fort ingénieuse.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Are there kids?</td>\n",
              "      <td>Y a-t-il des enfants ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Come in.</td>\n",
              "      <td>Entrez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Where's Boston?</td>\n",
              "      <td>Où est Boston ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You see what I mean?</td>\n",
              "      <td>Vous voyez ce que je veux dire ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c43aeb9-1327-40c5-9f5b-917843f83712')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c43aeb9-1327-40c5-9f5b-917843f83712 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c43aeb9-1327-40c5-9f5b-917843f83712');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2e13acee-0f7f-4c81-aed2-3ec2022c4220\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2e13acee-0f7f-4c81-aed2-3ec2022c4220')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2e13acee-0f7f-4c81-aed2-3ec2022c4220 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"en\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31447,\n        \"samples\": [\n          \"Do come by all means.\",\n          \"I need an envelope.\",\n          \"Bring Tom back.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 45097,\n        \"samples\": [\n          \"Il fuit.\",\n          \"Vous pouvez venir.\",\n          \"Je l'ai entendu sortir.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/no-more-low-resources-bengali-machine-translation/final_data.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:34:58.126785Z",
          "iopub.execute_input": "2024-05-08T10:34:58.128179Z",
          "iopub.status.idle": "2024-05-08T10:35:22.923756Z",
          "shell.execute_reply.started": "2024-05-08T10:34:58.128136Z",
          "shell.execute_reply": "2024-05-08T10:35:22.92273Z"
        },
        "trusted": true,
        "id": "pqTOXc6v5x_w",
        "outputId": "2761cd90-2a33-4809-a410-bf5f4fbedc2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/no-more-low-resources-bengali-machine-translation/final_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b9653d06f422>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/no-more-low-resources-bengali-machine-translation/final_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/no-more-low-resources-bengali-machine-translation/final_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(df.isnull().sum())\n",
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T16:32:06.442377Z",
          "iopub.execute_input": "2024-05-04T16:32:06.442813Z",
          "iopub.status.idle": "2024-05-04T16:32:13.971833Z",
          "shell.execute_reply.started": "2024-05-04T16:32:06.442782Z",
          "shell.execute_reply": "2024-05-04T16:32:13.970554Z"
        },
        "trusted": true,
        "id": "pjCn0EUz5x_x",
        "outputId": "ce9017ae-dd9e-46b9-de6d-c8ea559e20a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n",
            "en    0\n",
            "fr    0\n",
            "dtype: int64\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:36:15.576797Z",
          "iopub.execute_input": "2024-05-08T10:36:15.577192Z",
          "iopub.status.idle": "2024-05-08T10:36:16.584604Z",
          "shell.execute_reply.started": "2024-05-08T10:36:15.57716Z",
          "shell.execute_reply": "2024-05-08T10:36:16.583796Z"
        },
        "trusted": true,
        "id": "MFQbDJdi5x_x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(1000000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:36:18.253556Z",
          "iopub.execute_input": "2024-05-08T10:36:18.253902Z",
          "iopub.status.idle": "2024-05-08T10:36:18.258536Z",
          "shell.execute_reply.started": "2024-05-08T10:36:18.253873Z",
          "shell.execute_reply": "2024-05-08T10:36:18.257575Z"
        },
        "trusted": true,
        "id": "98xIImnJ5x_x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())\n",
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:32:23.007996Z",
          "iopub.execute_input": "2024-05-08T10:32:23.008374Z",
          "iopub.status.idle": "2024-05-08T10:32:24.767283Z",
          "shell.execute_reply.started": "2024-05-08T10:32:23.008343Z",
          "shell.execute_reply": "2024-05-08T10:32:24.766221Z"
        },
        "trusted": true,
        "id": "CU6c9WJq5x_y",
        "outputId": "2d66fbe6-8d62-4cfc-cea3-c53e189143e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en    0\n",
            "fr    0\n",
            "dtype: int64\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(15):\n",
        "    print(df['en'][i+1])\n",
        "    print(df['fr'][i+1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T12:13:07.45969Z",
          "iopub.execute_input": "2024-05-04T12:13:07.459992Z",
          "iopub.status.idle": "2024-05-04T12:13:07.477917Z",
          "shell.execute_reply.started": "2024-05-04T12:13:07.459965Z",
          "shell.execute_reply": "2024-05-04T12:13:07.476301Z"
        },
        "trusted": true,
        "id": "PHv0Op7c5x_y",
        "outputId": "09a1069d-cc9f-4780-f6c9-a775fafc210b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are there kids?\n",
            "Y a-t-il des enfants ?\n",
            "Come in.\n",
            "Entrez !\n",
            "Where's Boston?\n",
            "Où est Boston ?\n",
            "You see what I mean?\n",
            "Vous voyez ce que je veux dire ?\n",
            "I'm not a good loser.\n",
            "Je suis un mauvais perdant.\n",
            "I'm not bitter.\n",
            "Je ne suis pas amer.\n",
            "I'll call you at noon.\n",
            "Je vous appellerai à midi.\n",
            "What a dope!\n",
            "Quel couillon !\n",
            "Only time will tell.\n",
            "Seul le temps nous le dira.\n",
            "It looks really good.\n",
            "Ça a l'air vraiment bon.\n",
            "We obeyed the rules.\n",
            "Nous avons obéi au règlement.\n",
            "We must obey.\n",
            "Il nous faut obéir.\n",
            "Start now.\n",
            "Commence maintenant.\n",
            "I like mysteries.\n",
            "J'aime les mystères.\n",
            "I hope I'll fit in.\n",
            "J'espère que je m'intégrerai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "num_sentences = len(df)\n",
        "num_train = int(train_ratio * num_sentences)\n",
        "num_val = int(val_ratio * num_sentences)\n",
        "num_test = num_sentences - num_train - num_val\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:36:21.918823Z",
          "iopub.execute_input": "2024-05-08T10:36:21.919641Z",
          "iopub.status.idle": "2024-05-08T10:36:22.473188Z",
          "shell.execute_reply.started": "2024-05-08T10:36:21.919599Z",
          "shell.execute_reply": "2024-05-08T10:36:22.472083Z"
        },
        "trusted": true,
        "id": "lOQeeqv_5x_y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df[:num_train]\n",
        "val_df = df[num_train:num_train+num_val]\n",
        "test_df = df[num_train+num_val:]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:36:22.544686Z",
          "iopub.execute_input": "2024-05-08T10:36:22.545224Z",
          "iopub.status.idle": "2024-05-08T10:36:22.550053Z",
          "shell.execute_reply.started": "2024-05-08T10:36:22.545196Z",
          "shell.execute_reply": "2024-05-08T10:36:22.549032Z"
        },
        "trusted": true,
        "id": "rsIKzhE35x_z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "8_OG9nAx5x_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_en_tokens = []\n",
        "train_fr_tokens = []\n",
        "val_en_tokens = []\n",
        "val_fr_tokens = []\n",
        "test_en_tokens = []\n",
        "test_fr_tokens = []\n",
        "en_vocab = {'<pad>': 0}  # Initialize English vocabulary with <pad> token\n",
        "fr_vocab = {'<pad>': 0}  # Initialize Bengali vocabulary with <pad> token\n",
        "\n",
        "def tokenize_sentence(sentence, vocab):\n",
        "    tokens = sentence.split()\n",
        "    token_ids = []\n",
        "    for token in tokens:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)\n",
        "        token_ids.append(vocab[token])\n",
        "    return token_ids\n",
        "\n",
        "# Tokenizing training data\n",
        "print(\"Tokenizing training data:\")\n",
        "for en_sent, fr_sent in tqdm(zip(train_df['en'], train_df['fr']), total=len(train_df)):\n",
        "    train_en_tokens.append(tokenize_sentence(en_sent, en_vocab))\n",
        "    train_fr_tokens.append(tokenize_sentence(fr_sent, fr_vocab))\n",
        "\n",
        "# Tokenizing validation data\n",
        "print(\"Tokenizing validation data:\")\n",
        "for en_sent, fr_sent in tqdm(zip(val_df['en'], val_df['fr']), total=len(val_df)):\n",
        "    val_en_tokens.append(tokenize_sentence(en_sent, en_vocab))\n",
        "    val_fr_tokens.append(tokenize_sentence(fr_sent, fr_vocab))\n",
        "\n",
        "# Tokenizing testing data\n",
        "print(\"Tokenizing testing data:\")\n",
        "for en_sent, fr_sent in tqdm(zip(test_df['en'], test_df['fr']), total=len(test_df)):\n",
        "    test_en_tokens.append(tokenize_sentence(en_sent, en_vocab))\n",
        "    test_fr_tokens.append(tokenize_sentence(fr_sent, fr_vocab))\n",
        "\n",
        "# Update the vocabulary sizes\n",
        "src_vocab_size = len(en_vocab)\n",
        "tgt_vocab_size = len(fr_vocab)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:36:23.557251Z",
          "iopub.execute_input": "2024-05-08T10:36:23.558017Z",
          "iopub.status.idle": "2024-05-08T10:36:41.034631Z",
          "shell.execute_reply.started": "2024-05-08T10:36:23.557982Z",
          "shell.execute_reply": "2024-05-08T10:36:41.033644Z"
        },
        "trusted": true,
        "id": "cA697wp-5x_z",
        "outputId": "c419f10e-9abc-401d-a914-62e576a4d655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167,
          "referenced_widgets": [
            "b033917262cf4175a597b13dddc08766",
            "7147cb76c8c64132ab4f1f823d268ef3",
            "e404811732a4485e9586ac6e483b9c07",
            "7bd7e031e56a4a8bbc4dba215c1275cf",
            "229466c012354d899584b252ebd6fbe5",
            "945da085a3024159991ebcb9e075aa42",
            "071ff52b919f434cb03da907fea9e05d",
            "2e6835c94ffd48afa27fe7a4d17fcb04",
            "f6c150993ec04980a71db674a85c5ad7",
            "2066efac9f9343d5b090eb8f5c43d321",
            "9c066e16906e43f487382f6e5d1ce400",
            "ba32e2e0c22746b69c061c0c8754b1d5",
            "7f80bea5036f4eff977507b41b7b7b1c",
            "6560961986534e6094a3f75b033fbc57",
            "5c8d9624fa8e4ef4b4de39cd68d46d97",
            "c81aa6506ca74e68aac1120c499ae537",
            "b62af005dbab4b91a8a4d791ad89aa54",
            "d2a8839b7f8a4b7e9f2cc4e6b374027d",
            "eb7170f60b914db9b9ebbba8915a7d26",
            "5f083ca73c244585a6c01c9253397150",
            "0fd615bff20d49309d798d7b2fa949c7",
            "60902767caec4565b43c33aceaf3a732",
            "df8817c980b14ef1aa8e634e163c21d4",
            "ded55bc7b098477580bfe50ee8677454",
            "26316c32128844a0b0942a0fa221ecb7",
            "b5523bcf53ae4fd8afdd975346553926",
            "1dc61ef7cd744fd5baf13a37cb09435d",
            "371f4fd70b3242f08c4b5bcac2672d04",
            "5bece993c9394e7c9eba6bf696cdb2c3",
            "a0c46c88fbf84584b48ff90391dcf45d",
            "66cb1d1db5744bd1b0ba7f073a8bc5e0",
            "1816bf4d8db84af98a9f299c3ef394b2",
            "d164d527ca7945708a1e57a07a10d541"
          ]
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing training data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/40000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b033917262cf4175a597b13dddc08766"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing validation data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba32e2e0c22746b69c061c0c8754b1d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing testing data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df8817c980b14ef1aa8e634e163c21d4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custome Dataset"
      ],
      "metadata": {
        "id": "SKHzAhBC5x_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(data.Dataset):\n",
        "    def __init__(self, en_tokens, fr_tokens):\n",
        "        self.en_tokens = en_tokens\n",
        "        self.fr_tokens = fr_tokens\n",
        "        self.max_len = max(max(len(en), len(fr)) for en, fr in zip(en_tokens, fr_tokens))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en_tokens)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        en_data = self.en_tokens[index] + [0] * (self.max_len - len(self.en_tokens[index]))  # Padding with 0\n",
        "        fr_data = self.fr_tokens[index] + [0] * (self.max_len - len(self.fr_tokens[index]))  # Padding with 0\n",
        "        return torch.tensor(en_data), torch.tensor(fr_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:42:46.249257Z",
          "iopub.execute_input": "2024-05-08T10:42:46.24989Z",
          "iopub.status.idle": "2024-05-08T10:42:46.257084Z",
          "shell.execute_reply.started": "2024-05-08T10:42:46.249861Z",
          "shell.execute_reply": "2024-05-08T10:42:46.256122Z"
        },
        "trusted": true,
        "id": "mSWZuYx05x_z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(train_en_tokens, train_fr_tokens)\n",
        "val_dataset = TranslationDataset(val_en_tokens, val_fr_tokens)\n",
        "test_dataset = TranslationDataset(test_en_tokens, test_fr_tokens)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = data.DataLoader(val_dataset, batch_size=2)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:42:49.12892Z",
          "iopub.execute_input": "2024-05-08T10:42:49.129296Z",
          "iopub.status.idle": "2024-05-08T10:42:49.559107Z",
          "shell.execute_reply.started": "2024-05-08T10:42:49.129269Z",
          "shell.execute_reply": "2024-05-08T10:42:49.5583Z"
        },
        "trusted": true,
        "id": "6svnDovk5x_0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first batch of data from train_loader\n",
        "for batch_idx, (en_data, fr_data) in enumerate(train_loader):\n",
        "    print(\"English Data (Batch 0):\", en_data)\n",
        "    print(\"Bengali Data (Batch 0):\", fr_data)\n",
        "    break  # Break after printing the first batch to avoid printing the entire dataset\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:42:51.66008Z",
          "iopub.execute_input": "2024-05-08T10:42:51.660445Z",
          "iopub.status.idle": "2024-05-08T10:42:51.750803Z",
          "shell.execute_reply.started": "2024-05-08T10:42:51.660416Z",
          "shell.execute_reply": "2024-05-08T10:42:51.749694Z"
        },
        "trusted": true,
        "id": "bvHUHNhT5x_0",
        "outputId": "6b0953a7-8844-45bb-aca4-36405f2136d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Data (Batch 0): tensor([[ 196,  209, 3164,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 177,  730,    5,  125,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0]])\n",
            "Bengali Data (Batch 0): tensor([[ 141,  262,  411, 6403,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0],\n",
            "        [ 328, 1041, 1042,  406,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "\n",
        "<hr>\n",
        "<h4>Model Architecture</h4>\n",
        "<hr>\n",
        "<img src='https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png'>"
      ],
      "metadata": {
        "id": "-qB69SpU5x_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Step Wise Explanation:*\n",
        "* **input Embedding**: The process begins with encoding the input language(e.g English sequenc) into numerical vectors. Each word or token is transformed into a high-dimentional vector.\n",
        "\n",
        "* **Multi-Head Self-Attention:** This is the heart of the transformer. The model looks at each word in the input sentences and assings different lavels of inportance to other words in the sentence. Multiple attention heads allow the model to focus on different aspects of the sentence simultaneously.\n",
        "\n",
        "* **Positional Encoding:**  Since transformers don't have inherent sence of word order, positional encoding is added to the word embeddings to help the model understand the words's position in the sentence.\n",
        "\n",
        "* **Encoder - Decoder Architecture** : In translation task, there are typically two parts: the encoder and the decoder. The encoder takes the input sentence and process it, while the decoder generates the translated output.\n",
        "\n",
        "* **Decoder Self-Attention** : The decoder also uses multi-head self-attention, but slightly modified to prevent if from looking ahead in the output sentence, which would result in incorrect translations.\n",
        "\n",
        "* **Attention Output** : The outputs from the attention mechanisms are uesd to calculte attention scores, which datermine how much each word in the input sentence contributes to each word in the output sentence. Position-wise Feedforward Networks: After attention, the model passes the data through feedforward neural networks ot further process and refine the information. Output Layer: The final layer in the decoder procduces probablilities for each word in the targer language vaocabulary, allowing the model to predict the next word in the tanslationl.\n",
        "\n",
        "* **Training And Optimization** : Transformers are trained using large parralel corpora of source and targer language sentences. They learn to minimize the difference between predicted tranlations and the actual translations in the training data.\n",
        "\n",
        "* **Repeat For Each Token** : This process is repeated for each word in the output sentence, where the previously generated words are used as context for generating the next word. Beam Search or Greedy Decoding: During inference, the model generates translations one word at a time. Beam search or greedy decoding is often used to select the most likely next word based on the model's predictions."
      ],
      "metadata": {
        "id": "84JZxEM55x_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Inside Attention Layer</h4>\n",
        "<img src='https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png'>"
      ],
      "metadata": {
        "id": "RhCMMCu85x_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Easy to understand Explanation:\n",
        "\n",
        "Let's break down and realte it to the components and process in a transformer model:\n",
        "\n",
        "   *  School and Studens: Think of the school as the entire context, and the students as the individual tokens in a sequence.\n",
        "\n",
        "   *  Vecitorization and Tokenization : the process of converting students into tokens and vectorizing them represents in the initial preprocessing steps where text data is tokenized into individual words or tokens and then converted into numerical vector representations.\n",
        "\n",
        "   * Vocabulary: The vocabulary of the school represents the set of unique tokens (students) that the model has learned from previous schools within the same company. Thse tokens are sued to represent words in the sequence.\n",
        "\n",
        "   * Intrs-Attention (Self-Attention) : Each student's interaction with their classmates represents the  intra-attention mechanishm, where relationships, influences, and context between tokenss (students) are captured. Each studen becomes a query (Q), and their classmates become keys (K) and values (V). Attention scores are calculated to determine how much weight each student should give to their calssmates. Softmax normalization of attention scores can be thought of as grading each student's relationships and influence each others. Concatenation of information from different teachers (heads) captures diverse insights.\n",
        "\n",
        "   * Linear Layer: The linear layer represents the post-attention but post-attention precessing step that helps combine and refine information before producing the final output.\n",
        "\n",
        "\n",
        "This is the essence of how attention mechanishm work in t transformers, where tokens (students) atten to  each other, calculate their influence, and produce context vectors (mark sheets) for each other. These context vocetors are then in cross-attention to compare tokens from different parts of the model, ultimately leading to the model's final output\n",
        "\n",
        "Encoder's Role(intra-attention in encoder): the encoder preocess the input sequence and performs intra-attention. it produces context vectors (contextual representation) for each word in the input sequence. Thesee context vecotrs capture information about how each word related to others within input sequence\n",
        "\n",
        "Signaling the Decoder: The decoder is signaled to start generating the output sequence. Typically , this is fone by providing the decoder with an intial input, often a special start token (e.g ,or).\n",
        "\n",
        "Generating the first Word: For the first word in the output sequence, the decoder combines the follwing: The start token as the initail query. The encoder's context vectors, which represent the input sequence. The decoder's own context vector for the output sequence (initialzed explicitly). These componentes are used to predicte the word in the output sequence.\n",
        "\n",
        "Subssequent Word Preditions: For genrating subsequent words in the output sequence, the following process occurs: The shifted target (previously generated word) becomes the query. The encodr's enctext vector, representing the input sequence, are used for context. The context vectors for the target word (which includes context from the encoder) are also considered. The last word's hidden state, obtained from the decoder's self-attention (intra-attention), is incorporated. These components collectively contribute to the prediction of each subsequent word in the output sequence.\n",
        "\n",
        "Ierative toekn Generation: The decoder repeats the process of generating tokens one by one, considering context from both the encoder's input sequence and it's own generated sequence. At each step, the decoder calculates a probability distribution over the vocabulary for the next token and selects the token with the highest probability.\n",
        "\n",
        "Ending the sequence: The process continues until the model generates an end token or  reaches a predefined maximum sequence length"
      ],
      "metadata": {
        "id": "j6GE-LSV5x_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Transformer Architecture\n",
        "\n",
        " <img src=\"https://www.mihaileric.com/static/feedforward_layer_and_normalization-dfdcfbd00009f7f99eca73ae29f2dfb7-4ec3a.png\">"
      ],
      "metadata": {
        "id": "JUr-HMIa5x_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "XTcJ08IW5x_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:37:18.176477Z",
          "iopub.execute_input": "2024-05-08T10:37:18.177296Z",
          "iopub.status.idle": "2024-05-08T10:37:18.182979Z",
          "shell.execute_reply.started": "2024-05-08T10:37:18.177265Z",
          "shell.execute_reply": "2024-05-08T10:37:18.182007Z"
        },
        "trusted": true,
        "id": "6rbw6vxF5x_1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TqutW88t5x_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:37:18.865309Z",
          "iopub.execute_input": "2024-05-08T10:37:18.86566Z",
          "iopub.status.idle": "2024-05-08T10:37:18.873895Z",
          "shell.execute_reply.started": "2024-05-08T10:37:18.86563Z",
          "shell.execute_reply": "2024-05-08T10:37:18.872643Z"
        },
        "trusted": true,
        "id": "fJuQzw1V5x_1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custome Attention Layer"
      ],
      "metadata": {
        "id": "lsPOrTK55x_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Multi-Head-Attention Layer**\n",
        "<hr>"
      ],
      "metadata": {
        "id": "M6G-1i1Y5x_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, debug_str = None):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.debug_str = debug_str\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if (self.debug_str == 'cross'):\n",
        "            print('attn_scores:',attn_scores.shape, mask.shape)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:37:19.667587Z",
          "iopub.execute_input": "2024-05-08T10:37:19.668662Z",
          "iopub.status.idle": "2024-05-08T10:37:19.687582Z",
          "shell.execute_reply.started": "2024-05-08T10:37:19.668611Z",
          "shell.execute_reply": "2024-05-08T10:37:19.685968Z"
        },
        "trusted": true,
        "id": "Ck6qiF875x_1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer"
      ],
      "metadata": {
        "id": "MOCqsfWm5x_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:37:20.126578Z",
          "iopub.execute_input": "2024-05-08T10:37:20.126932Z",
          "iopub.status.idle": "2024-05-08T10:37:20.134561Z",
          "shell.execute_reply.started": "2024-05-08T10:37:20.126902Z",
          "shell.execute_reply": "2024-05-08T10:37:20.133508Z"
        },
        "trusted": true,
        "id": "hFfzuE0h5x_2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "dIEFzIwx5x_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)#, debug_str=\"cross\")\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:37:20.708534Z",
          "iopub.execute_input": "2024-05-08T10:37:20.709366Z",
          "iopub.status.idle": "2024-05-08T10:37:20.72009Z",
          "shell.execute_reply.started": "2024-05-08T10:37:20.709323Z",
          "shell.execute_reply": "2024-05-08T10:37:20.718881Z"
        },
        "trusted": true,
        "id": "pCHqV_zD5x_2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As you see the combined mask does not consider the pad tokens.\n",
        "Lets define each of the mask roles:\n",
        "- Padding mask - removes consideration of unnecessary pad tokens.\n",
        "- Causal mask - Prevents from peeking in the future and helps decoder in predicting one token at a time.\n",
        "- Cross-Attention mask - In the context of cross-attention between the encoder and decoder, a mask is used to ensure that the decoder only attends to positions in the encoder that have valid information. In this case, it can be similar to a padding mask when dealing with sequences of different lengths.\n",
        "- Combined mask - takes the best of both world and  It ensures that the decoder doesn't include padding tokens in its consideration (like the padding mask) and enforces the autoregressive behavior (like the causal mask), allowing the decoder to predict one token at a time while avoiding future tokens.\n"
      ],
      "metadata": {
        "id": "8nqAU3ee5x_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Transformer Model"
      ],
      "metadata": {
        "id": "ZB1sKQpa5x_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, pad_token_src = 0, pad_token_tgt = 0, device = 'cpu'):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.pad_token_src = pad_token_src\n",
        "        self.pad_token_tgt = pad_token_tgt\n",
        "        self.device = device\n",
        "        self = self.to(self.device)\n",
        "\n",
        "    def generate_mask(self, src_mask, tgt_mask):\n",
        "        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt_mask.size(2)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask.to(self.device)\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def decode(self, src, bos_token_id, eos_token_id, mask=None, max_dec_length = 25):\n",
        "        \"\"\"\n",
        "        for inference\n",
        "        Args:\n",
        "            src: input to encoder\n",
        "            trg: input to decoder\n",
        "        out:\n",
        "            out_labels : returns final prediction of sequence\n",
        "        \"\"\"\n",
        "\n",
        "        tgt = torch.tensor([[bos_token_id]]*src.shape[0]).to(self.device)\n",
        "        if mask:\n",
        "            src_mask, tgt_mask = self.generate_mask(mask['src_mask'], mask['tgt_mask'])\n",
        "        else:\n",
        "            src_mask, tgt_mask = self.generate_mask(src!=self.pad_token_src, tgt!=self.pad_token_tgt)\n",
        "\n",
        "        enc_output = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        out_labels = tgt\n",
        "        unfinished_seq = np.array([1]*src.shape[0])\n",
        "        i=0;\n",
        "        while (sum(unfinished_seq)>0 & i<max_dec_length):\n",
        "            dec_output = self.dropout(self.positional_encoding(self.decoder_embedding(out_labels)))\n",
        "            for dec_layer in self.decoder_layers:\n",
        "                dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "            output = self.fc(dec_output)\n",
        "\n",
        "            out_labels = torch.cat((out_labels, output[:,-1:,:].argmax(-1)),dim=1)\n",
        "\n",
        "            unfinished_seq[(out_labels[:,-1] == eos_token_id).cpu().numpy()] = 0\n",
        "\n",
        "            i += 1;\n",
        "        return out_labels\n",
        "\n",
        "    def forward(self, src, tgt, mask = None):\n",
        "        if mask:\n",
        "            src_mask, tgt_mask = self.generate_mask(mask['src_mask'], mask['tgt_mask'])\n",
        "        else:\n",
        "            src_mask, tgt_mask = self.generate_mask(src!=self.pad_token_src, tgt!=self.pad_token_tgt)\n",
        "\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:37:21.662281Z",
          "iopub.execute_input": "2024-05-08T10:37:21.663054Z",
          "iopub.status.idle": "2024-05-08T10:37:21.68498Z",
          "shell.execute_reply.started": "2024-05-08T10:37:21.663014Z",
          "shell.execute_reply": "2024-05-08T10:37:21.683932Z"
        },
        "trusted": true,
        "id": "J69HuMke5x_3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# Hyperparameters\n",
        "src_vocab_size = len(en_vocab)\n",
        "tgt_vocab_size = len(fr_vocab)\n",
        "# Reduce Model Size\n",
        "d_model = 64  # Decrease the model dimensionality\n",
        "num_heads = 2  # Decrease the number of attention heads\n",
        "num_layers = 2  # Decrease the number of layers\n",
        "d_ff = 512  # Decrease the size of the feed-forward layers\n",
        "max_seq_length = max(train_dataset.max_len, val_dataset.max_len, test_dataset.max_len)  # Maximum sequence length\n",
        "dropout = 0.1  # Dropout probability\n",
        "\n",
        "# Instantiate the Transformer model\n",
        "transformer_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, device = device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:37:22.327319Z",
          "iopub.execute_input": "2024-05-08T10:37:22.328014Z",
          "iopub.status.idle": "2024-05-08T10:37:23.509315Z",
          "shell.execute_reply.started": "2024-05-08T10:37:22.327982Z",
          "shell.execute_reply": "2024-05-08T10:37:23.508219Z"
        },
        "trusted": true,
        "id": "mXhJ6kHT5x_3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformer_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:33:21.883784Z",
          "iopub.execute_input": "2024-05-08T10:33:21.884469Z",
          "iopub.status.idle": "2024-05-08T10:33:21.890222Z",
          "shell.execute_reply.started": "2024-05-08T10:33:21.884434Z",
          "shell.execute_reply": "2024-05-08T10:33:21.889165Z"
        },
        "trusted": true,
        "id": "dqnLqd2S5x_3",
        "outputId": "b346bdbf-ca28-4884-fc4d-85fff3492914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder_embedding): Embedding(9859, 64)\n",
            "  (decoder_embedding): Embedding(17415, 64)\n",
            "  (positional_encoding): PositionalEncoding()\n",
            "  (encoder_layers): ModuleList(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (feed_forward): PositionWiseFeedForward(\n",
            "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): EncoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (feed_forward): PositionWiseFeedForward(\n",
            "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (decoder_layers): ModuleList(\n",
            "    (0): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (cross_attn): MultiHeadAttention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (feed_forward): PositionWiseFeedForward(\n",
            "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (cross_attn): MultiHeadAttention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (feed_forward): PositionWiseFeedForward(\n",
            "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
            "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
            "        (relu): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=64, out_features=17415, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 1\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    transformer_model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    # Create a progress bar\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch_idx, (src, tgt) in progress_bar:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
        "\n",
        "        # Flatten the output and target tensors to compute loss\n",
        "        output_flat = output.view(-1, output.size(-1))\n",
        "        tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output_flat, tgt_flat)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add batch loss to total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update progress bar description\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    transformer_model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Create a progress bar for validation\n",
        "        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\", unit=\"batch\")\n",
        "\n",
        "        for batch_idx, (src, tgt) in val_progress_bar:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
        "\n",
        "            # Flatten the output and target tensors to compute loss\n",
        "            output_flat = output.view(-1, output.size(-1))\n",
        "            tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output_flat, tgt_flat)\n",
        "\n",
        "            # Add batch loss to total loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Update progress bar description\n",
        "            val_progress_bar.set_postfix({\"Validation Loss\": loss.item()})\n",
        "\n",
        "    # Calculate average validation lfrom tqdm import tqdm\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    transformer_model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    # Create a progress bar\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch_idx, (src, tgt) in progress_bar:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
        "\n",
        "        # Flatten the output and target tensors to compute loss\n",
        "        output_flat = output.view(-1, output.size(-1))\n",
        "        tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output_flat, tgt_flat)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add batch loss to total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update progress bar description\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    transformer_model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Create a progress bar for validation\n",
        "        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\", unit=\"batch\")\n",
        "\n",
        "        for batch_idx, (src, tgt) in val_progress_bar:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
        "\n",
        "            # Flatten the output and target tensors to compute loss\n",
        "            output_flat = output.view(-1, output.size(-1))\n",
        "            tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output_flat, tgt_flat)\n",
        "\n",
        "            # Add batch loss to total loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Update progress bar description\n",
        "            val_progress_bar.set_postfix({\"Validation Loss\": loss.item()})\n",
        "\n",
        "    # Calculate average validation loss\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(transformer_model.state_dict(), 'transformer_model.pth')\n",
        "avg_val_loss = val_loss / len(val_loader)\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(transformer_model.state_dict(), 'transformer_model.pth')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:43:34.356882Z",
          "iopub.execute_input": "2024-05-08T10:43:34.357345Z"
        },
        "trusted": true,
        "id": "DvZ_9OGw5x_8",
        "outputId": "de6dce00-875d-47f9-c3a5-5df5bf1095b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  44%|████▎     | 8733/20000 [12:46<20:48,  9.03batch/s, Loss=1.9]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T10:21:50.842937Z",
          "iopub.execute_input": "2024-05-08T10:21:50.843366Z",
          "iopub.status.idle": "2024-05-08T10:21:50.850207Z",
          "shell.execute_reply.started": "2024-05-08T10:21:50.843333Z",
          "shell.execute_reply": "2024-05-08T10:21:50.848809Z"
        },
        "trusted": true,
        "id": "Ur4wEUf45x_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save the model"
      ],
      "metadata": {
        "id": "Op6gMh7-5x_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"./transformer_overfit.pth\"\n",
        "# PATH = f\"./transformer_epoch_{epoch}_batch_{batch}.pth\""
      ],
      "metadata": {
        "id": "q2sq_0du5x_9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "apAgNspd5x_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "def translate_english_to_bengali(model, src_text, en_vocab, fr_vocab, device='cpu', max_length=50):\n",
        "    # Tokenize the input English text\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    src_tokens = tokenizer(src_text)\n",
        "\n",
        "    # Convert tokens to indices using the English vocabulary\n",
        "    src_indices = [en_vocab[token] for token in src_tokens]\n",
        "\n",
        "    # Convert indices to tensor and add batch dimension\n",
        "    src_tensor = torch.tensor(src_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate mask for the source input\n",
        "    src_mask = (src_tensor != model.pad_token_src).to(device)\n",
        "\n",
        "    # Translate the English text to Bengali\n",
        "    with torch.no_grad():\n",
        "        # Generate the translation\n",
        "        translation_tensor = model.decode(src_tensor, en_vocab['<bos>'], en_vocab['<eos>'], mask={'src_mask': src_mask})\n",
        "\n",
        "    # Convert translation tensor to list of indices\n",
        "    translation_indices = translation_tensor.squeeze(0).cpu().tolist()\n",
        "\n",
        "    # Convert indices to Bengali tokens\n",
        "    translation_tokens = [fr_vocab.itos[idx] for idx in translation_indices]\n",
        "\n",
        "    # Remove special tokens and return the translated text\n",
        "    return ' '.join(token for token in translation_tokens[1:] if token not in ['<eos>', '<pad>'])\n"
      ],
      "metadata": {
        "id": "qbGRDQ6s5x_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, device=device)\n",
        "model.load_state_dict(torch.load('transformer_model.pth'))\n",
        "model.eval()\n",
        "en_vocab = len(en_vocab)\n",
        "fr_vocab = len(fr_vocab)\n",
        "\n",
        "# Enter the English text to translate\n",
        "english_text = \"Enter your English text here.\"\n",
        "\n",
        "# Translate the English text to Bengali\n",
        "bengali_text = translate_english_to_bengali(model, english_text, en_vocab, fr_vocab, device=device)\n",
        "\n",
        "# Print the translated text\n",
        "print(\"Translated Bengali text:\", bengali_text)\n"
      ],
      "metadata": {
        "id": "O_j6WeE75x_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}